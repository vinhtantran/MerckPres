---
title: "<small>Presentation at Merck</small><br/><br/>An Overview of Monothetic Cluster Analysis and Two Collaboration Projects"
author: "Tan Tran, PhD"
date: "September 12, 2019"
bibliography: "mybib.bib"
biblio-style: "apalike"
link-citations: true
output:
  xaringan::moon_reader:
    css: [default, "msu.css", tamu-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = FALSE, message = FALSE, warning = FALSE,
               comment = NA, fig.path = 'figure/', cache.path = 'cache/', 
               out.width='100%', fig.width = 6, fig.retina = 2)

options(htmltools.dir.version = FALSE)
```

```{r load}
library(RefManageR)
library(tidyverse)
library(gridExtra)
library(RColorBrewer)
library(monoClust)
library(dendextend) # for extending Ward's dengrogram plots
library(kableExtra)
library(circular)
library(leaflet)
library(lubridate)
library(PULS)
library(fda)
library(fpc)

# Always use select() from dplyr package instead of MASS
select <- dplyr::select

theme_set(theme_bw(base_size = 14) +
            theme(legend.position = "bottom",
                  strip.background = element_rect(fill = "white", linetype = "blank"),
                  strip.text = element_text(hjust = 0.05)))
set.seed(1234)

BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = "authoryear", 
           style = "markdown",
           max.names = 3,
           longnamesfirst = FALSE,
           hyperlink = FALSE, 
           dashed = FALSE)
bib <- ReadBib("mybib.bib", check = FALSE)
```

# Introduction
## Clustering

* Unsupervised learning techniques for grouping (multivariate) responses with the goal of:
  * homogeneity — internal cohesion
  * separation — external isolation
  
* When to use clustering:
  * Find underlying patterns where little or no information about the data are known or to compare to known groups
  * Prediction of cluster membership based on the common characteristics of the clusters
  
* "A classification of a set of objects is not like a scientific theory and should perhaps be judged largely on its usefulness [...]." `r Citep(bib, "Everitt2011a")`

???

Test the font size

---
## Two Clustering Techniques

* Optimization clustering techniques

  * The number of clusters, $K$, have to be pre-determined
  * Move the objects between clusters as long as it improves the criterion
  * $k$-means and partitioning around medoids (PAM, or $k$-medoids) are two examples of this technique
  
* Hierarchical clustering techniques

  * Distance measures between objects and between clusters must be defined
    * single linkage, complete linkage, Ward's method, etc.
  * Objects are fused together (agglomerative), or separated from each 
other (divisive) in each step based on the distance metric
  * The result is usually presented by dendrogram

---
## An Example: $k$-Means

Data from `r TextCite(bib, "Barbour2017")` cerebro-spinal fluid (CSF) biomarker data 
set with $n = 225$ subjects. The variables of interest are the standardized log ratios of 22 biomarkers (proteins).
* Multiple Sclerosis (MS) and non-MS patients are known
* $Q = 2$ proteins are displayed to visually demonstrate the method

```{r kmeanhier, out.width="45%", fig.show="hold", fig.width=6, fig.height=6, fig.align='center'}
brewer2 <- brewer.pal(n = 2, "Dark2")[1:2]
brewer3 <- brewer.pal(n = 3, "Dark2")

health <- read_csv("../../../2018-2019Fall/Dissertation_Tan/supportingCode/presentation/data/DataForTan_12052017.csv")
health.2.ratio <- health %>%
  select(3, 4) %>% mutate_all(log) %>% mutate_all(scale) %>% mutate_all(as.numeric)

kmeans.health <- kmeans(health.2.ratio, centers = 2)
kmeans.membership <- ifelse(kmeans.health$cluster == 2, 1, 2)
kmeans.centroid <- tibble(`SL004672/SL003322` = kmeans.health$centers[,1],
                          `SL000467/SL004857` = kmeans.health$centers[,2])
kmeans.health.plot <- ggplot(health.2.ratio, aes(x = `SL004672/SL003322`, 
                                                 y = `SL000467/SL004857`, 
                                                 color = factor(kmeans.membership))) + 
  geom_point(aes(shape = health$current_ms), size=3) + 
  geom_point(data = kmeans.centroid, color = "#7570b3", size = 5) +
  scale_color_manual(guide = F, values = brewer2[2:1]) + 
  scale_shape_discrete(name = "Disease condition") +
  labs(title = "k-means on CSF data",
       x = "log(SL004672/SL003322)",
       y = "log(SL000467/SL004857)")

kmeans.health.plot
```

---

## An Example: Hierarchical with Ward's Method

```{r hclust, out.width="50%", fig.show="hold", fig.width=6, fig.height=6}
hclust.health <- hclust(d = dist(health.2.ratio), method = "ward.D2")

hclust.health %>%
  as.dendrogram %>%
  color_branches(k = 2, col = brewer2) %>%
  as.ggdend() %>%
  ggplot(labels = F) +
  theme_void()

hclust.mem.health <- cutree(hclust.health, k = 2)
hclust.mem.health <- ifelse(hclust.mem.health == 2, 1, 2)
hclustplot <- ggplot(health.2.ratio, aes(x = `SL004672/SL003322`,
                                         y = `SL000467/SL004857`,
                                         color = factor(hclust.mem.health))) +
  geom_point(aes(shape = health$current_ms), size=3) +
  scale_color_manual(guide = F, values = brewer2) +
  scale_shape_discrete(name = "Disease condition") +
  labs(title = "Ward's method on CSF data",
       x = "log(SL004672/SL003322)",
       y = "log(SL000467/SL004857)")
hclustplot
```

---

## Polythetic vs. Monothetic Clustering

* Popular methods like k-means and Ward's are **polythetic methods**
  * Clustered using the combined information of variables
  * Observations in a cluster are similar "on average" but may share no common characteristics

* There are also **monothetic divisive methods**
  * Data are bi-partitioned based on values of one variable at a time
  * Observations share common characteristics: in the same interval or category

---

## Monothetic Clustering Algorithm

* Introduced in `r TextCite(bib, "Chavent1998")` and `r TextCite(bib, "Piccarreta2007")`, inspired by classification and regression trees `r Citep(bib, "Breiman1984")`

* A global criterion called **inertia** for a cluster $C_k$ is defined as 
$$I(C_k) = \frac{1}{n_k} \sum_{(i, j) \in C_k, i > j}  d^2(\mathbf{y_i},\mathbf{y_j})$$
where $d(\mathbf{y_i},\mathbf{y_j})$ is the distance between observations $\mathbf{y_i}$ and $\mathbf{y_j}$ and $n_k$ is the cluster size

* Let $s$ be a binary split dividing a cluster $C_k$ into two clusters $C_{kL}$ and $C_{kR}$. The decrease in inertia is 
$$\Delta (s, C_k) = I(C_k) - I(C_{kL}) - I(C_{kR})$$

* The best split is selected as
$$s^*(C_k) = \arg \max_s {\Delta I(s,C_k)} $$


---

## Properties of Monothetic Clustering
* Inertia is a global optimization criterion

* Bi-partition observations based on one variable at a time, making the method monothetic

* Defines rules for cluster membership
  * Easy classification of new members
  
* For the CSF data, monothetic clustering can be useful to allow classification into groups with shared characteristics that *might* relate to disease presence/absence

???

Shared characteristics in the second bullet.

---

## Monothetic Clustering on the CSF Data

```{r monodemo2, out.width="50%", fig.show="hold", fig.width=6, fig.height=6}
health.2.ratio.mono <- data.frame(health.2.ratio)
colnames(health.2.ratio.mono) <- c("rat1", "rat2")
mono.health.2 <- MonoClust(health.2.ratio.mono, nclusters = 2)
# Tree
plot(mono.health.2, cols = brewer2, margin = 0.2)

# Scatterplot
mono.mem.2 <- mono.health.2$Membership - 1

mono2.medoid <- tibble(`SL004672/SL003322` = health.2.ratio.mono[c(169, 161), 1],
                       `SL000467/SL004857` = health.2.ratio.mono[c(169, 161), 2])

# Plot with two clusters first
monoplot.2 <- ggplot(health.2.ratio, aes(x = `SL004672/SL003322`, 
                                       y = `SL000467/SL004857`, 
                                       color = factor(mono.mem.2))) +
    geom_point(aes(shape = health$current_ms), size=3) +  
    geom_point(data = mono2.medoid, color = "#7570b3", size = 5) +
    scale_color_manual(guide = F, values = brewer2) + 
    scale_shape_discrete(name = "Disease condition") +
    geom_vline(xintercept = -0.2642147, color = "red") +
    labs(title = "Monothetic Clustering on CSF Data",
       x = "log(SL004672/SL003322)",
       y = "log(SL000467/SL004857)")

monoplot.2
```

???

QUICK!!!

---

## One more split
```{r monodemo3, out.width="50%", fig.show="hold", fig.width=6, fig.height=6}
mono.health.3 <- MonoClust(health.2.ratio.mono, nclusters = 3)
plot(mono.health.3, cols = brewer.pal(3, "Dark2"), margin = 0.2)

# Scatterplot
mono.mem.3 <- as.numeric(factor(mono.health.3$Membership))
mono3.medoid <- tibble(`SL004672/SL003322` = health.2.ratio.mono[c(169, 104, 141), 1],
                       `SL000467/SL004857` = health.2.ratio.mono[c(169, 104, 141), 2])

monoplot.3 <- ggplot(health.2.ratio, aes(x = `SL004672/SL003322`, 
                                       y = `SL000467/SL004857`, 
                                       color = factor(mono.mem.3))) +
    geom_point(aes(shape = health$current_ms), size=3) +  
    geom_point(data = mono3.medoid, color = "#7570b3", size = 4) +
    scale_color_discrete(guide=F) + 
    scale_shape_discrete(name = "Disease condition") +
    geom_vline(xintercept = -0.2642147, color = "red") +
    labs(title = "Monothetic Clustering on CSF data",
       x = "log(SL004672/SL003322)",
       y = "log(SL000467/SL004857)") +
    geom_segment(aes(x = -0.2642147, 
                   y = 0.37791, 
                   xend = max(`SL004672/SL003322`), 
                   yend = 0.37791), color = "blue")

monoplot.3
```

???

QUICK!!!

---
```{r loadfunctional, echo = FALSE}
north2018 <- read_csv(file = "../../../2018-2019Fall/Dissertation_Tan/supportingCode/functional/data/N_seaice_extent_daily_v3.0.csv")

north <- north2018[-1,] %>% 
  dplyr::select(Year, Month, Day, Extent) %>%
  mutate(Year = parse_double(Year),
         Month = parse_double(Month),
         Day = parse_double(Day),
         Extent = parse_double(Extent)) %>%
  mutate(yday = yday(make_date(Year, Month, Day))) %>%
  dplyr::select(Year, yday, Extent)

# Problematic years
problems <- c(1978, 1987, 1988, 2019)

# 1982 1990 2006 2018
withheld_years <- c(1982, 1990, 2006, 2018)

# Add some data before and after the current years to avoid overfitting at ends,
# adding 30 days before and after. On the way, I trimmed the days of year to 365.

north.tails <- north %>%
  filter(yday %in% 1:365) %>%
  filter(yday < 30 | yday > 336) %>%
  mutate(Year = ifelse(yday < 60, Year - 1, Year + 1)) %>%
  mutate(yday = ifelse(yday > 60, yday - 365, yday + 365))

north.extra <- north %>%
  bind_rows(north.tails) %>%
  arrange(Year, yday)

north.extra <- north.extra %>%
  filter(!(Year %in% problems),
         !(Year %in% withheld_years))

year.lst <- unique(north.extra[["Year"]])
predicted.mat <- readRDS("../../../2018-2019Fall/Dissertation_Tan/supportingCode/functional/data/predicted.mat.rds")
predicted.extent <- as.vector(t(predicted.mat))
new.arctic.full <- tibble(Extent = predicted.extent, 
                          Year = rep(year.lst, each = 366), 
                          yday = rep(1:366, length(year.lst)))

Jan<-c(1,31)
Feb<-c(31,59)
Mar<-c(59,90)
Apr<-c(90,120)
May<-c(120,151)
Jun<-c(151,181)
Jul<-c(181,212)
Aug<-c(212,243)
Sep<-c(243,273)
Oct<-c(273,304)
Nov<-c(304,334)
Dec<-c(334,365)

intervals<-rbind(Jan,Feb,Mar,Apr,May,Jun,Jul,Aug,Sep,Oct,Nov,Dec)

```
# Clustering Functional Data
## Arctic Sea Ice Extent Data

* Arctic Sea ice extent data set has been collected by National Snow & Ice Data Center since November 1978 `r Citep(bib, "NSIDC2018")`

```{r sat, fig.height=4, fig.width=8, out.width="80%", fig.align='center'}
ggplot(north) + 
  geom_linerange(aes(x = yday, ymin=Year-0.2, ymax = Year+0.2), 
                 size=0.5, color = "red") +
  scale_y_continuous(breaks = seq(1980, 2020, by = 5), minor_breaks = NULL) +
  xlab("Day") + ylab("Year")
```

???

1987 change

---

## Raw Arctic Ice Extent

```{r iceextent, fig.width=10, fig.height=6, out.width="100%"}
ggplot(north) + 
  geom_line(aes(x = yday, y = Extent, group = Year, color=Year)) +
  xlab("Day") + ylab("Arctic Ice Extent (mils of sq km2)") +
  scale_x_continuous(breaks = unique(as.vector(intervals)), minor_breaks = NULL) +
  geom_vline(xintercept = unique(as.vector(intervals)), linetype="dashed", color = "darkblue") +
  annotate("text", x=mean(intervals[1,]), y=5, label="Jan") +
  annotate("text", x=mean(intervals[2,]),y=5, label="Feb") +
  annotate("text", x=mean(intervals[3,]),y=5, label="Mar") +
  annotate("text", x=mean(intervals[4,]),y=5, label="Apr") +
  annotate("text", x=mean(intervals[5,]),y=5, label="May") +
  annotate("text", x=mean(intervals[6,]),y=15, label="Jun") +
  annotate("text", x=mean(intervals[7,]),y=15, label="Jul") +
  annotate("text", x=mean(intervals[8,]),y=15, label="Aug") +
  annotate("text", x=mean(intervals[9,]),y=15, label="Sep") +
  annotate("text", x=mean(intervals[10,]),y=15, label="Oct") +
  annotate("text", x=mean(intervals[11,]),y=15, label="Nov") +
  annotate("text", x=mean(intervals[12,]),y=15, label="Dec") +
  scale_color_continuous(breaks = c(1978, 2018))
```

???

Decreasing trend AND seasonal, dip...

---
## Functional Data

* When measurements were taken over some ordered index, such as time, frequency, or space `r Citep(bib, "Ramsay2005")`
  * Responses are continuous as a function of the index
  * Possibly high frequency of observations and smooth underlying process or observations
  
* Observations are converted to functional data using basis functions:
  * **B-splines basis**, Fourier basis, Wavelets, etc.
  
* Penalized B-splines with knots at every day optimized with cross-validation for each curve

* Ice extent area in a year can be expressed as a function of time, $y_i(t)$
  * where $i$ is the year, $t$ is the day of year, and $y$ is the ice extent at that time point

---

## Smoothed and Interpolated Curves

```{r smootheny, fig.width=10, fig.height=6}
small.year <- c(1980, 2010)
north.nomissing <- north %>%
  filter(!(Year %in% problems))
new.arctic.small <- new.arctic.full %>%
  filter(Year %in% small.year)
raw.arctic.small <- north.nomissing %>%
  filter(Year %in% small.year)

combined.small <- bind_rows(raw.arctic.small, new.arctic.small) %>%
  mutate(smooth = c(rep("Unsmoothed", nrow(raw.arctic.small)), rep("Smoothed", nrow(new.arctic.small)))) %>%
  mutate(smooth = factor(smooth, levels=c("Unsmoothed", "Smoothed")))

ggplot(combined.small, aes(x = yday, y = Extent, group = interaction(Year, smooth), color=factor(Year))) + 
  geom_line() +
  geom_point(size = 1) +
  labs(x = "Day", 
       y = "Arctic Ice Extent (mils of sq km2)",
       color = "Year") +
  facet_grid(smooth ~  .)
```

???

Penalized B-splines with knots at every day to optimized for **each curve**

---

## Clustering Functional Data

* The $L_2$ distance matrix can be be calculated and used for non-functional clustering algorithms
    $$d(y_i, y_j) = \sqrt{\int_T [y_i(t) – y_j(t)]^2 dt}$$

* Monothetic clustering uses functional data in their discretized form
  * Transform the data into functional presentations
  * Data are then estimated to a common fine grid of $t$, $y_{it}$ from $y_i(t)$
  * Missing values are imputed and the data set is balanced with $t = 1, \ldots, 366$ days for all years

---
## Monothetic Clustering Result

* Evaluate functional data for each year, cluster years 
    using the 366 "variables" (days) each year
* Splitting the data based on one variable (day) at a time
* Have to deal with many equivalent splits possible at "neighboring" 
    variables    
    
```{r monosplit, out.width="50%", fig.show='hold', fig.height=6, fig.width=8}
# Do the 
new.arctic <- data.frame(predicted.mat)
rownames(new.arctic) <- as.character(unique(north.extra$Year))
colnames(new.arctic) <- as.character(1:366)

out4 <- MonoClust(new.arctic, nclusters=4, digits = 2)
# tree.with.p <- perm.test(out4, new.arctic)
colorbr <- RColorBrewer::brewer.pal(n = 4, name = "PuOr")
plot(out4, cols = colorbr)

maxvalue <- max(new.arctic.full$Extent)
minvalue <- min(new.arctic.full$Extent)

splits <- tibble(x = c(1, 1, 186, 186, 210, 210),
                 y = c(13.37, 13.37, 10.56, 10.56, 7.03, 7.03),
                 yend = c(minvalue, maxvalue, 6, maxvalue, minvalue, 14),
                 order = c(4, 7, 6, 7, 4, 5))

years <- unique(north.extra$Year)
medoids.years.mono <- years[out4$medoids]

dataplot <- new.arctic.full %>%
  filter(Year %in% unique(north.extra$Year)) %>%
  mutate(group = rep(out4$Membership, each = 366))

ggplot(dataplot) + 
  geom_line(aes(x = yday, y = Extent, group = Year, color=factor(group)), 
            size = 0.5, alpha = 0.4) +
  geom_line(data = dataplot %>% filter(Year %in% medoids.years.mono),
            aes(x = yday, y = Extent, group = Year, color=factor(group)), size = 1) +
  geom_segment(data = splits, 
               aes(x = x, xend = x, y = y, 
                   yend = yend, color = factor(order)),
               size = 1) +
  labs(x = "Day",
       y = "Arctic Ice Extent (mils of sq km2)",
       color = "Cluster") +
  scale_x_continuous(breaks = unique(as.vector(intervals)), minor_breaks = NULL) +
  scale_color_brewer(palette="PuOr",
                     labels = c("Low in Jan, low in Jul",
                                  "Low in Jan, high in Jul",
                                  "High in Jan, low in Jul",
                                  "High in Jan, high in Jul")) +
  guides(colour = guide_legend(nrow = 2)) +
  annotate("text", x=mean(intervals[1,]), y=10, label="Primary split") +
  annotate("text", x=mean(intervals[7,]), y=13, label="Secondary splits") +
  annotate("text", x=mean(intervals[1,]), y=5, label="Jan", alpha = 0.5) +
  annotate("text", x=mean(intervals[7,]), y=15, label="Jul", alpha = 0.5)# +
  #coord_polar()
```

???

* Reason: because so smooth, close points have same info
* Plot: High Jan, Low Jan, then High/low July and High/low July

---
## Partitioning Using Local Subregions

* By aggregating over regions of time, we develop a new clustering 
    algorithm related to monothetic clustering that is more suited to 
    functional data
* PULS recursively bi-partitions functional data using only groups 
    from subregions.
* For each subregion $[a_1, b_1], \ldots, [a_R, b_R]$, calculate an $L_2$ 
    distance matrix
    $$d_R(y_i, y_j) = \sqrt{\int_{a_r}^{b_r} [y_i(t) – y_j(t)]^2 dt},$$
* Apply a clustering algorithm (PAM, Ward's method, etc.) to create 
    2-group cluster solutions in $R$ subregions
* Pick the solution that maximizes the difference in the global inertia 
* Recursively apply to the newly created clusters
    
---
## Partitioning Using Local Subregions Result

```{r step6}

#Build common argval fd object from predicted.mat
NBASIS <- 300
NORDER <- 4
y<-t(predicted.mat)
splinebasis <- create.bspline.basis(rangeval=c(1, 366),nbasis=NBASIS,norder=NORDER)
fdParobj<-fdPar(fdobj=splinebasis,Lfdobj=2,lambda=.000001) # No need for any more smoothing
yfd.full<-smooth.basis(argvals=1:366, y=y, fdParobj=fdParobj)

yfd.train<-yfd.full
```

```{r pulssplit, out.width="50%", fig.show='hold', fig.height=6, fig.width=8}
PULS4.pam <- PULS(toclust.fd=yfd.train$fd,intervals=intervals,nclusters=4,method = "pam")
plot(PULS4.pam, cols = colorbr[c(3,4,1,2)])

medoids.years.puls <- unique(north.extra$Year)[PULS4.pam$frame$medoid[4:7]]
colnames(yfd.train$y) <- unique(north.extra$Year)
dataplot.temp <- as_tibble(yfd.train$y) 

dataplot <- dataplot.temp %>%
  gather(key = Year, value = Extent) %>%
  mutate(group = rep(PULS4.pam$Membership, each = 366),
         yday = rep(1:366, length(unique(north.extra$Year))))

ggplot(dataplot) + 
  geom_line(aes(x = yday, y = Extent, group = Year, color=factor(group)), 
            size = 0.5, alpha = 0.4) +
  geom_line(data = dataplot %>% filter(Year %in% medoids.years.puls),
            aes(x = yday, y = Extent, group = Year, color=factor(group)), size = 1) +
  labs(x = "Day",
       y = "Arctic Ice Extent (mils of sq km2)") +
  scale_x_continuous(breaks = unique(as.vector(intervals)), minor_breaks = NULL) +
  scale_color_brewer(palette="PuOr") +
  theme(legend.position = "none") +
  annotate("rect", xmin = 181, xmax = 243, ymin=minvalue, ymax = maxvalue, alpha = 0.2, fill = "gray") +
  annotate("text", x=mean(intervals[7,]), y=15, label="Jul", alpha = 0.5) +
  annotate("text", x=mean(intervals[8,]), y=15, label="Aug", alpha = 0.5) #+
  #coord_polar()
```

---
## Comparison of Results and Cluster Prediction

```{r compare}
clustertab <- tibble(Name = c("High Jan, High Jul",
                              "High Jan, High Jul",
                              "High Jan, Low Jul",
                              "High Jan, Low Jul",
                              "Low Jan, High Jul",
                              "Low Jan, Low Jul"),
  PULS = c("1979-.red[**1981**], 1983-1984,",
                              "1986, 1989, 1992, 1994, 1996",
                              "1985, 1991, 1993, 1995,", 
                              ".red[**1997**]-2004",
                              "2005, 2008-2010, .red[**2013**], 2014",
                              "2007, .red[**2011**], 2012, 2015-2017"),
                     MonoClust = c("1979-1981, 1983-1986, 1989,", 
                                   "1992-.red[**1994**], 1996",
                               "1991, 1995,", 
                               "1997-.red[**2000**]-2004",
                               "2005, 2008-2010, .red[**2013**], 2014",
                               "2007, .red[**2011**], 2012, 2015-2017")
                     )

clustertab %>%
  kable(escape = F) %>%
  collapse_rows(columns = 1, valign = "top")
```

???

* Medoid year in bold
* Very similar, last two clusters are identical
* 1985, 1993 (1) to (PULS 2)

--

* One year in each decade was randomly withheld from the test data set: 
  * `r paste(withheld_years, collapse=", ")`
* Predict the cluster from monothetic clustering's splitting rule tree

```{r sortin}
# 1982 --> high high
y1982 <- north %>%
  filter(Year %in% 1982,
         yday %in% c(2, 186, 210)) %>%
  mutate(Observed = paste0("Day", yday, " = ", Extent)) %>%
  group_by(Year) %>%
  summarize("Observed Extent" = paste(Observed, collapse = "; "))

# 1990 --> high low
y1990 <- north %>%
  filter(Year %in% 1990,
         yday %in% c(1, 186, 210)) %>%
  mutate(Observed = paste0("Day", yday, " = ", Extent)) %>%
  group_by(Year) %>%
  summarize("Observed Extent" = paste(Observed, collapse = "; "))

# 2006 --> low high
y2006 <- north %>%
  filter(Year %in% 2006,
         yday %in% c(1, 186, 210)) %>%
  mutate(Observed = paste0("Day", yday, " = ", Extent)) %>%
  group_by(Year) %>%
  summarize("Observed Extent" = paste(Observed, collapse = "; "))

# 2018 --> low low
y2018 <- north %>%
  filter(Year %in% 2018,
         yday %in% c(1, 186, 210)) %>%
  mutate(Observed = paste0("Day", yday, " = ", Extent)) %>%
  group_by(Year) %>%
  summarize("Observed Extent" = paste(Observed, collapse = "; "))

bind_rows(y1982, y1990, y2006, y2018) %>%
  mutate(Cluster = c("High Jan, High Jul",
                     "High Jan, Low Jul",
                     "Low Jan, High Jul",
                     "Low Jan, Low Jan")) %>%
  select(Cluster, Year) %>%
  spread(key = Cluster, value = Year) %>%
  kable()
```

???

* Monothetic clustering has clear rule so can predict

---

## Simulation Study
* Compare the performance of various clustering techniques on functional data
  * Monothetic clustering, PAM, and Ward's method
  * PULS with PAM and PULS with Ward's method on on five subregions
  
```{r simulcurves, results='hide', fig.width=10, fig.height=4}
fda_data <- readRDS("../../../2018-2019Fall/Dissertation_Tan/supportingCode/functional/fda_data_sample.RDS")
trueCluster <- readRDS("../../../2018-2019Fall/Dissertation_Tan/supportingCode/functional/fda_trueClust.RDS")

# plot(fda_data$fd, col=trueCluster, xlab = "t", ylab = "y(t)")

library(stringr)
str_locate("aaa12xxx", "[0-9]+")
#      start end
# [1,]     4   5
str_extract("aaa12xxx", "[0-9]+")

fda_data$fd$coefs %>%
  as_tibble() %>%
  mutate(time = seq(0, 100.5, by = 0.5)) %>%
  gather(key = "x", value = "y", -time) %>%
  mutate(x.num = as.numeric(str_extract(x, "[0-9]+")),
         truecluster = trueCluster[x.num]) %>%
  ggplot(aes(x = time, y = y, group = x, col = factor(truecluster))) +
  geom_line(alpha = 0.75) +
  scale_x_continuous(breaks = seq(0, 100, 20), limits = c(0, 100)) +
  scale_color_brewer(palette = "Dark2") +
  theme(panel.grid.major.x =  element_line(color = "black", linetype = "dashed"),
        legend.position = "none")
```

* Corrected Rand Index `r Citep(bib, "Rand1971")` and Pseudo-R2 are used to evaluate
* PULS and monothetic clustering both perform better than PAM and competitive with Ward's method
* Ward's method works well both within PULS and by itself

---


## Conclusions 

* Monothetic clustering bi-partitions data at the values of one variable at a time

  * Clusters share the same characteristics from split variables
  * Can interpret the resulting clusters
  * Can predict the cluster a new observation would fall into
  
* R packages `monoclust` and `PULS` are available on Github

--

## Possible Extensions

* Sparse clustering `r Citep(bib, "Witten2010")` can be applied to functional data to "weight" the contributions of variables (days) to the clustering process

* Monothetic Exhaustive search is slow, heuristic search algorithms may help with some trade-offs
  * Results of best split for each variable could be stored so they won't unnecessarily repeat

* Clustering the derivative of the Arctic ice extent data

---
# Statistical Consultation Collaborations: Power Analysis

* A research grant proposal to the USDA ARS Pulse Crop Health Initiative Research Project Plan

* Principal Investigator: Mary Miles, PhD, FACSM, of Montana State University

* My role: Statistician, involved in designing the analysis plan and statistical power analysis for the project

* Was aprroved for funding last week

---
## Objectives

1. **Determine the impact of pulse (green lentil and black bean) consumption on postprandial triglyceride (TG) and inflammation responses to a high-fat meal challenge.**

2. Determine the extent to which the gut microbiome and changes in the gut microbiome induced by pulse consumption influence health impacts 

3. Measure metabolomic profiles to elucidate underlying mechanisms linking pulse consumption to improved health.

---
## Study Plan

![](figure/studyplan1.png)

---
## Collaboration

1. Learn the study
  * Biological terminologies
  * Microbiome data analysis
  * Metabolomics data analysis

2. Prioritize the objective
  * Objective 1
  * Objectives 2 and 3 are explanatory
  
3. Design the analysis plan
  * Obj. 1: one-sided t-test with possible multiple testings
  * Obj. 2: PLS-DA and variable importance to specify species differentiate the two treatment groups
  * Obj. 3: untargeted analysis using XCMS and mummichog
  
4. Power analysis based on Objective 1
  * Budget limit: can only recruit 36 to 48 people

---
## Determine the Effect Size

* There was a pilot study with similar design
  * Two groups of participants: high fat responder (n = 22) and low fat responder (n = 18)
  * The high fat meal challenge was done and the change in triglyceride levels were measured several times
  * There was no 12-week experiment
  
```{r generalplot, echo = FALSE, fig.width=10, fig.height=4}
tmao <- read_csv("../../data-temp/Gut Microbiome Data/participantdata.csv")

tmao_TGR <- tmao %>%
  mutate(TGR0 = 0,
         TGR1 = TG_1 - TG_0,
         TGR2 = TG_2 - TG_0,
         TGR3 = TG_3 - TG_0,
         TGR4 = TG_4 - TG_0) %>%
  mutate(TGR_max = pmax(TGR1, TGR2, TGR3, TGR4),
         TGR_Group = ifelse(TGR_max > 60, "High", "Low"))

tmao_TGR %>% 
  select("#NAME", TGR0:TGR4, TGR_max, FatResponder) %>%
  gather(key = time, value = TG, TGR0:TGR4) %>%
  mutate(time = fct_recode(factor(time),
                           "0" = "TGR0",
                           "1" = "TGR1",
                           "2" = "TGR2",
                           "3" = "TGR3",
                           "4" = "TGR4"),
         time = as.numeric(as.character(time)),
         FatResponder = fct_recode(FatResponder,
                                   "Fat Responder: High" = "High",
                                   "Fat Responder: Low" = "Low")) %>%
  ggplot(aes(x = time, y = TG, group = `#NAME`, color = FatResponder)) +
  # geom_point(alpha = 0.5) +
  geom_line(alpha = 0.8) +
  facet_wrap(~ FatResponder) +
  theme(legend.position = "none") +
  labs(y = expression(paste(Delta, "TGR")),
       x = "Hour",
       title = "Changes of Triglyceride Compared to Pre-Meal Level")
```

---
## Max Triglyceride

```{r maxplot, echo = FALSE, fig.width=10, fig.height=3}
tmao_TGR %>% 
  select(TGR_max, FatResponder) %>%
  bind_rows(tmao_TGR %>% select(TGR_max, FatResponder) %>% mutate(FatResponder = "Combined")) %>% 
  ggplot(aes(x = FatResponder, y = TGR_max, color = FatResponder)) +
  geom_violin(trim = FALSE, scale = "count", width = 0.45) +
  geom_point(alpha = 0.5, position = position_jitter(width = 0.05)) +
  stat_summary(fun.y = "mean", geom = "crossbar",
               aes(ymin = ..y.., ymax = ..y..), width = 0.5) +
  scale_x_discrete(limits = c("Low", "High", "Combined")) +
  scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "none") +
  labs(y = "Max TGR",
       x = "Fat Responder",
       title = "Maximum Triglyceride Levels")
```

```{r summarytab, results = 'hide'}
hilo <- mosaic::favstats(tmao_TGR$TGR_max)
hi <- mosaic::favstats(tmao_TGR %>% filter(TGR_Group == "High") %>% pull(TGR_max))
lo <- mosaic::favstats(tmao_TGR %>% filter(TGR_Group == "Low") %>% pull(TGR_max))

bind_rows(lo, hi, hilo) %>%
  add_column(sample = c("Low Responders Only",
                        "High Responders Only",
                        "Combined")) %>% 
  select(sample, everything()) %>%
  kable() %>% 
  kable_styling("striped")
```

```{r effectsize}
bind_rows(hilo, hi) %>%
  add_column(sample = c("High and Low Responders", "High Responders Only")) %>% 
  mutate(mean2 = mean/2, 
         effect = (mean - mean2)/sd) %>%
  select(sample, mean, mean2, sd, effect) %>%
  rename(`Mean` = mean,
         `Expected Mean After (50%)` = mean2,
         `SD` = sd,
         `Effect size` = effect) %>%
  kable(digits = 2) %>% 
  kable_styling("striped", full_width = FALSE)
```

---
## Power Analysis Plot with Effect Size d = 0.54

```{r obj1a, fig.width=10, fig.height=6, echo = F}
library(plotly)
library(pwr)
# Effect size is calculated based on the change in TG_1 and TG_0 divided by
# SE of the TG_0
no_of_vars <- rep(rep(1:7, each = 21), 4) # 7

# power <- rep(seq(0.2, 0.9, 0.05), 7*4)

d <- rep(seq(0.5, 0.8, 0.1), each = 21 * 7) # 4

n <- rep(10:30, 4 * 7) # 21

options <- tibble(no_of_vars, power = NA, d, n = n)

for (i in 1:nrow(options)) {
  options$power[i] = pwr.t.test(d = 0.5,
                            n = options$n[i],
                            sig.level = 0.05/options$no_of_vars[i], 
                            type = c("two.sample"),
                            alternative = "greater")$power
}



# n <- map2_dbl(power, no_of_vars, function(x, y) pwr.t.test(d = 0.7, sig.level = 0.1/y, power = x, type = c("two.sample"))$n)

powerplot_n <- options %>%
  ggplot(aes(x = n, y = power, group = factor(no_of_vars), color = factor(no_of_vars))) +
  geom_line() +
  geom_segment(x = 0, xend = 150, y = 0.8, yend = 0.8, linetype = 2, color = 'blue3') +
  geom_segment(x = 18, xend = 18, y = 0, yend = 1, linetype = 2, color = 'red3') +
  geom_segment(x = 24, xend = 24, y = 0, yend = 1, linetype = 2, color = 'red3') +
  # geom_segment(x = 6, xend = 6, y = 0, yend = 3, linetype = 2, color = 'red3') +
  scale_x_continuous(breaks = seq(12, 30, 6)) +
  scale_color_brewer(palette = "Set1") +
  labs(title = "One-sided two sample t-test\nFamily-wise significance level = 0.05",
       x = "Sample size (each group)",
       y = "Power",
       color = "Number of variables\n(multiple tests)")

powerplot_n
# ggplotly(powerplot_n)
```

---
## Another Power Analysis Calculation with d = 0.8

* Another meeting happened and the researcher agreed to screen the participants further to not include people with low high fat responder.

```{r obj1c, fig.width=10, fig.height=6, echo = F}
# Effect size is calculated based on the change in TG_1 and TG_0 divided by
# SE of the TG_0
no_of_vars <- rep(rep(1:7, each = 21), 4) # 7

# power <- rep(seq(0.2, 0.9, 0.05), 7*4)

d <- rep(seq(0.5, 0.8, 0.1), each = 21 * 7) # 4

n <- rep(10:30, 4 * 7) # 21

options <- tibble(no_of_vars, power = NA, d, n = n)

for (i in 1:nrow(options)) {
  options$power[i] = pwr.t.test(d = 0.8,
                            n = options$n[i],
                            sig.level = 0.05/options$no_of_vars[i], 
                            type = c("two.sample"),
                            alternative = "greater")$power
}



# n <- map2_dbl(power, no_of_vars, function(x, y) pwr.t.test(d = 0.7, sig.level = 0.1/y, power = x, type = c("two.sample"))$n)

powerplot_n <- options %>%
  ggplot(aes(x = n, y = power, group = factor(no_of_vars), color = factor(no_of_vars))) +
  geom_line() +
  geom_segment(x = 0, xend = 150, y = 0.8, yend = 0.8, linetype = 2, color = 'blue3') +
  geom_segment(x = 18, xend = 18, y = 0, yend = 1, linetype = 2, color = 'red3') +
  geom_segment(x = 24, xend = 24, y = 0, yend = 1, linetype = 2, color = 'red3') +
  # geom_segment(x = 6, xend = 6, y = 0, yend = 3, linetype = 2, color = 'red3') +
  scale_x_continuous(breaks = seq(12, 30, 6)) +
  scale_color_brewer(palette = "Set1") +
  labs(title = "One-sided two sample t-test\nFamily-wise significance level = 0.05",
       x = "Sample size (each group)",
       y = "Power",
       color = "Number of variables\n(multiple tests)")

powerplot_n
# ggplotly(powerplot_n)
```

---
## Deliverables

* An interactive report to assist the researcher to decide the sample size and the power of the test.
